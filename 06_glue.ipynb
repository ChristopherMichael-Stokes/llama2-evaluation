{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2 import *\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import string\n",
    "from evaluate import evaluator\n",
    "from evaluate import load\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_size='int4'\n",
    "max_samples=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOADING_MODEL\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = LlamaModel(\n",
    "    model_name=model_name,\n",
    "    model_resolution=model_size\n",
    ")\n",
    "\n",
    "model.model.to = lambda x: x # Disable device copying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=LOADING_DATA\n",
    "dataset = datasets.load_dataset('glue', 'mnli', split='validation_matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Define data prep and model inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(sample: dict) -> str:\n",
    "    \"\"\"Format a sample from the squad V2 dataset to question answer string.\"\"\"\n",
    "    pretext = (f'{model.B_INST} You are performing natural language inference tasks. '\n",
    "               'Given a premise and hypothesis, decide whether we have an entailment, neutral or contradiction relation. '\n",
    "               'Only respond with the words \"The response is \" and one of \"neutral, contradiction, entailment\"'\n",
    "               f' {model.E_INST}\\n')  #Â Llama system directive\n",
    "    q_a = (f'Premise: {sample[\"premise\"]}\\n'\n",
    "         f'Hypothesis: {sample[\"hypothesis\"]}\\n'\n",
    "         f'Answer: \\n\\nThe response is:')\n",
    "    \n",
    "    return pretext + q_a\n",
    "\n",
    "\n",
    "def glue_inference(df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    \"\"\"Predict the output extracts for all samples in the input squad format dataset\"\"\"\n",
    "    df_val = df.copy(deep=True)\n",
    "    df['prediction'] = -1\n",
    "    df['prob'] = -1\n",
    "    \n",
    "    labels = ['neutral', 'entailment', 'contradiction']\n",
    "    label_ids = [model.tokenizer.encode(label, add_special_tokens=False)[0] for label in labels]\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "        with torch.no_grad():\n",
    "            x = format_question(df.iloc[idx])\n",
    "            tokens = model.tokenize(x)\n",
    "            logits = model.model(tokens).logits\n",
    "            probs = torch.softmax(logits[:,-1,label_ids], dim=1)\n",
    "            \n",
    "        df_val.loc[idx, 'prediction'] = probs.argmax().item()\n",
    "        df_val.loc[idx, 'prob'] = probs.max().item()\n",
    "        \n",
    "    df_val.prediction = df_val.prediction.astype(int)\n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=RUNNING_INFERENCE\n",
    "pd_dataset = dataset.to_pandas()\n",
    "if max_samples > 0 and max_samples < len(pd_dataset):\n",
    "    pd_dataset = pd_dataset.iloc[:max_samples - 1]\n",
    "df2 = glue_inference(pd_dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=EVALUATION\n",
    "glue_metric = load(\"glue\", \"mnli_matched\")\n",
    "\n",
    "for precision in np.linspace(0,1,10, endpoint=False):\n",
    "    df_filt = df2[df2.prob > precision]\n",
    "    \n",
    "    predictions = df_filt['prediction'].to_list()\n",
    "    answers = df_filt['label'].to_list()\n",
    "\n",
    "    results = glue_metric.compute(predictions=predictions, references=answers)\n",
    "    print(f'Class prob > {precision:.1f}: {results}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
